---
title: Boosting Algorithm (AdaBoost and XGBoost)
author: Ajeng Prastiwi, David Tahi Ulubalang
github: https://github.com/ajengprstw/xgboost
date: '2020-11-23'
slug: xgboost
categories:
  - R
tags: 
  - Machine Learning
description: ''
featured: 'boosting-algorithm.png'
featuredalt: ''
featuredpath: 'date'
linktitle: ''
type: post
---



<style>
body {
text-align: justify}
</style>
<div id="introduction" class="section level1 tabset">
<h1>Introduction</h1>
<div id="what-is-boosting" class="section level2">
<h2>What is boosting?</h2>
<p>Boosting is an <strong>ensemble method</strong> of converting weak learners into strong learners. Weak and strong refer to a measure how correlated are the learners to the actual target variable[^1]. In boosting, each training sample are used to train one unit of decision tree and picked with replacement over-weighted data. The trees will learn from predecessors and updates the residuals error.</p>
</div>
<div id="learning-objectives" class="section level2">
<h2>Learning Objectives</h2>
<p>The goal of this article is to help you:</p>
<ul>
<li>Understand the concept of boosting<br />
</li>
<li>Compare boosting and bagging method<br />
</li>
<li>Understand how AdaBoost algorithm works<br />
</li>
<li>Understand how XGBoost algorithm works<br />
</li>
<li>Implement AdaBoost and Xgboost in business case</li>
</ul>
<center>
<img src="/img/xgboost/concept_map.png" />
</center>
</div>
<div id="library-and-setup" class="section level2">
<h2>Library and setup</h2>
<pre class="r"><code>library(tidyverse)
library(rsample)
library(xgboost)
library(ggthemes)
library(tictoc)
library(fastAdaboost)
library(tidymodels)
library(inspectdf)
library(caret)
theme_set(theme_pander())</code></pre>
</div>
</div>
<div id="bagging-vs-boosting" class="section level1">
<h1>Bagging vs Boosting</h1>
<p>The idea of bagging is creating many subsets of training sample with replacement, each observation has the same probability to picked as sample. Then, each training sample are used to train one unit of decision tree and use the average of all the predictions. In boosting, each training sample are used to train one unit of decision tree and picked with replacement over-weighted data. The trees will learn from predecessors and updates the residuals error. After these weak learners are trained, a weighted average of their estimates are taken for the final predictions at the end[^2].</p>
<center>
<img src="/img/xgboost/bagging.png" />
</center>
<center>
<img src="/img/xgboost/boostingg.png" />
</center>
</div>
<div id="boosting-method" class="section level1">
<h1>Boosting Method</h1>
<p>The different method of boosting algorithm are “How they create the weak learners during the iterative process”:</p>
<div id="adaboost" class="section level2">
<h2>AdaBoost</h2>
<p>Adaptive boosting was formulated by Yoav Freund and Robet Schapire. AdaBoost was the first practical boosting algorithm, and remains one of the most widely used and studied, with applications in numerous fields. AdaBoost algorithm works on changes the sample distribution by modifying weight data points for each iteration.</p>
<div id="how-adaboost-works" class="section level3">
<h3>How AdaBoost Works?</h3>
<p>We can split the idea of AdaBoost into 3 big concept :</p>
<div id="used-stump-as-weak-learners" class="section level4">
<h4>1. Used Stump as Weak Learners</h4>
<p>Weak learners is any model that has a accuracy better than random guessing even if it is just slightly better (e.g 0.51). In an Ensemble methods we combines multiple weak learners to make strong learner model. In AdaBoost, weak learners are used, a 1-level decision tree (Stump).The main idea when creating a weak classifier is to find the best stump that can separate data by minimizing overall errors.</p>
<center>
<img src="/img/xgboost/ada1.png" />
</center>
</div>
<div id="influence-the-next-stump" class="section level4">
<h4>2. Influence the Next Stump</h4>
<p>Unlike bagging, which makes models in parallel, Boosting does training sequentially, which means that each stump (weak learner) is affected by the previous stump. The way Stump affects the next stump is by giving different weights to the data that will be used in the next stump maknig process. This weighting is based on error calculations, if a data is incorrectly predicted in the first stump, then the data will be given a greater weight in the next stump-making process.</p>
<p><img src="/img/xgboost/ada2.png" /></p>
</div>
<div id="weighted-vote" class="section level4">
<h4>3. Weighted Vote</h4>
<p>In AdaBoost algorithm, each stump has a different weight, the weight for each stump is based on the resulting error rate. The smaller errors generated by a stump, the greater the weight of the stump. The weight of each stump is used in the voting process, if the greater the total weight obtained by one of the classes, then that class will be used as the final class.</p>
<p><img src="/img/xgboost/ada3.png" /></p>
</div>
</div>
<div id="case-example-using-adaboost" class="section level3">
<h3>Case Example using AdaBoost</h3>
<p>The hotel is one of the lodgings most often used when traveling. With limited hotel capacity, canceling a reservation can be detrimental to the person providing the hotel service. In this case, we will predict hotel cancellations using data <a href="https://www.kaggle.com/jessemostipak/hotel-booking-demand">Hotel Reservation Requests</a> taken from Kaggle.</p>
<pre class="r"><code>booking &lt;- read.csv(&quot;data_input/xgboost/hotel_bookings.csv&quot;, stringsAsFactors = T) 
head(booking)</code></pre>
<pre><code>#&gt;          hotel is_canceled lead_time arrival_date_year arrival_date_month
#&gt; 1 Resort Hotel           0       342              2015               July
#&gt; 2 Resort Hotel           0       737              2015               July
#&gt; 3 Resort Hotel           0         7              2015               July
#&gt; 4 Resort Hotel           0        13              2015               July
#&gt; 5 Resort Hotel           0        14              2015               July
#&gt; 6 Resort Hotel           0        14              2015               July
#&gt;   arrival_date_week_number arrival_date_day_of_month stays_in_weekend_nights
#&gt; 1                       27                         1                       0
#&gt; 2                       27                         1                       0
#&gt; 3                       27                         1                       0
#&gt; 4                       27                         1                       0
#&gt; 5                       27                         1                       0
#&gt; 6                       27                         1                       0
#&gt;   stays_in_week_nights adults children babies meal country market_segment
#&gt; 1                    0      2        0      0   BB     PRT         Direct
#&gt; 2                    0      2        0      0   BB     PRT         Direct
#&gt; 3                    1      1        0      0   BB     GBR         Direct
#&gt; 4                    1      1        0      0   BB     GBR      Corporate
#&gt; 5                    2      2        0      0   BB     GBR      Online TA
#&gt; 6                    2      2        0      0   BB     GBR      Online TA
#&gt;   distribution_channel is_repeated_guest previous_cancellations
#&gt; 1               Direct                 0                      0
#&gt; 2               Direct                 0                      0
#&gt; 3               Direct                 0                      0
#&gt; 4            Corporate                 0                      0
#&gt; 5                TA/TO                 0                      0
#&gt; 6                TA/TO                 0                      0
#&gt;   previous_bookings_not_canceled reserved_room_type assigned_room_type
#&gt; 1                              0                  C                  C
#&gt; 2                              0                  C                  C
#&gt; 3                              0                  A                  C
#&gt; 4                              0                  A                  A
#&gt; 5                              0                  A                  A
#&gt; 6                              0                  A                  A
#&gt;   booking_changes deposit_type agent company days_in_waiting_list customer_type
#&gt; 1               3   No Deposit  NULL    NULL                    0     Transient
#&gt; 2               4   No Deposit  NULL    NULL                    0     Transient
#&gt; 3               0   No Deposit  NULL    NULL                    0     Transient
#&gt; 4               0   No Deposit   304    NULL                    0     Transient
#&gt; 5               0   No Deposit   240    NULL                    0     Transient
#&gt; 6               0   No Deposit   240    NULL                    0     Transient
#&gt;   adr required_car_parking_spaces total_of_special_requests reservation_status
#&gt; 1   0                           0                         0          Check-Out
#&gt; 2   0                           0                         0          Check-Out
#&gt; 3  75                           0                         0          Check-Out
#&gt; 4  75                           0                         0          Check-Out
#&gt; 5  98                           0                         1          Check-Out
#&gt; 6  98                           0                         1          Check-Out
#&gt;   reservation_status_date
#&gt; 1              2015-07-01
#&gt; 2              2015-07-01
#&gt; 3              2015-07-02
#&gt; 4              2015-07-02
#&gt; 5              2015-07-03
#&gt; 6              2015-07-03</code></pre>
<p>The data contains 119390 observations and 32 variables. Here some description of each feature:</p>
<ul>
<li><p><code>hotel</code>: Hotel (H1 = Resort Hotel or H2 = City Hotel)</p></li>
<li><p><code>is_canceled</code>: Value indicating if the booking was canceled (1) or not (0)</p></li>
<li><p><code>lead_time</code>: Number of days that elapses between the entering date of the booking into the PMS and the arrival date</p></li>
<li><p><code>arrival_date_year</code>: Year of arrival date</p></li>
<li><p><code>arrival_date_month</code>: Month of arrival data</p></li>
<li><p><code>arrival_date_week_number</code>: Week number of year for arrival date</p></li>
<li><p><code>arrival_date_day_of_month</code>: Day of arrival date</p></li>
<li><p><code>stays_in_weekend_nights</code>: Number of week nights (Monday to Friday) the guest stayed or booked to stay at the hotel</p></li>
<li><p><code>adults</code>: Number of adults</p></li>
<li><p><code>children</code>: Number of children</p></li>
<li><p><code>babies</code>: Number of babies</p></li>
<li><code>meal</code>: Type of meal booked. Categories are presented in standard hospitality meal packages:
<ul>
<li>Undefined/SC : no meal package;</li>
<li>BB : Bed &amp; Breakfast;</li>
<li>HB : Half board (breakfast and one other meal-usually dinner);</li>
<li>FB : Full board (breakfast, lunch, and dinner)</li>
</ul></li>
<li><p><code>country</code>: Country of origin. Categories are represented in the ISO 3155-3:2013 format</p></li>
<li><p><code>market_segment</code>: Market segment designation. In categories, the term “TA” means “Travel agents” and “TO” means “Tour Operators”</p></li>
<li><p><code>distribution_channel</code>: Booking distribution channel. The term “TA” means “Travel Agents” and “TO” means “Tour Operators”</p></li>
<li><p><code>is_repeated_guest</code>: Value indicating if the booking name was from a repeated guest (1) or not (0)</p></li>
<li><p><code>previous_cancellations</code>: Number of previous bookings that were cancelled bu the customer prior to the current booking</p></li>
<li><p><code>previous_bookings_not_canceled</code>: Number of previous bookings not cancelled by the customer prior to the current booking</p></li>
<li><p><code>reserved_room_type</code>: Code of room type reserved. Code is represented instead of designation for anonymity reasons</p></li>
<li><p><code>assigned_room_type</code>: Code for the type of room assigned to the booking. Sometimes the assigned room type differs from the reserved room type due to hotel opeartions reasons (e.g overbooking) or by customer request. Code is presented instead of designation for anonymity reasons</p></li>
<li><p><code>booking_changes</code>: Number of changes/amendments made to the booking from the moment the booking was entered on the PMS until the moment of check-in cancellation</p></li>
<li><code>deposit_type</code>: Indication on if the customer made a deposit to guarantee the booking. This variable can assume three categories:
<ul>
<li>No deposit - no deposit was made;</li>
<li>Non refund - a deposit was made in the value of the total stay cost;</li>
<li>Refundable - a deposit was made with a value under the total cost of stay</li>
</ul></li>
<li><p><code>agent</code>: ID of the travel agency that made the booking</p></li>
<li><p><code>company</code>: ID of the company/entity that made the booking or responsible for paying the booking. ID is presented instad of designation for anonymity reasons</p></li>
<li><p><code>days_in_waiting_list</code>: Number of days the booking was in the waiting list before it was confirmed to the customer</p></li>
<li><code>customer_type</code>: Type of booking, assuming one of four categories:
<ul>
<li>Contract - when the booking has an allotment or other type of contract associated to it;</li>
<li>Group - when the booking is associated to a group;</li>
<li>Transient - when the booking is not part of a group or contract, and is not associated to other transient booking</li>
<li>Transient-party - when the booking is transient, but is associated to at least other transient booking</li>
</ul></li>
<li><p><code>adr</code>: Average daily rate as defined by dividing the sum of all lodging transactions by the total number of staying nights</p></li>
<li><p><code>required_car_parking_spaces</code>: Number of car parking spaces required by the customer</p></li>
<li><p><code>total_of_special_requests</code>: Number of special requests made by the customer (e.g. twin bed or high floor)</p></li>
<li><code>reservation_status</code>: Reservation las status, assuming one of three categories:
<ul>
<li>Canceled - booking was canceled by the customer;</li>
<li>Check-out - customer has checked in but already departed;</li>
<li>No-Show - customer did not check-in and did inform the hotel of the reason why</li>
</ul></li>
<li><p><code>reservation_status_date</code>: Date a which the last status was set. This variable can be used in conjuction with the reservation status to understand when was the booking canceled or when did the customer checked-out of the model.</p></li>
</ul>
<p>The model prediction will help hotel to predict the guest will <strong>cancel or not cancel</strong> the booking hotel. We will remove variables <code>agent</code> and <code>company</code> because there are have a lot of levels, and also we remove <code>reservation_status</code> and <code>reservation_status_date</code>.</p>
<pre class="r"><code>booking &lt;- booking %&gt;% 
          select(-c(reservation_status_date, agent, company,
                    reservation_status)) %&gt;% 
          mutate(is_canceled = as.factor(is_canceled))</code></pre>
<div id="exploratory-data-analysis" class="section level4">
<h4>Exploratory Data Analysis</h4>
<p>Before we go further, we need to check if there are any missing values in data. We can used <code>inspect_na()</code> function from <code>inspectdf</code> package to check the missing value.</p>
<pre class="r"><code>booking %&gt;% 
  inspect_na()</code></pre>
<pre><code>#&gt; # A tibble: 28 x 3
#&gt;    col_name                    cnt    pcnt
#&gt;    &lt;chr&gt;                     &lt;int&gt;   &lt;dbl&gt;
#&gt;  1 children                      4 0.00335
#&gt;  2 hotel                         0 0      
#&gt;  3 is_canceled                   0 0      
#&gt;  4 lead_time                     0 0      
#&gt;  5 arrival_date_year             0 0      
#&gt;  6 arrival_date_month            0 0      
#&gt;  7 arrival_date_week_number      0 0      
#&gt;  8 arrival_date_day_of_month     0 0      
#&gt;  9 stays_in_weekend_nights       0 0      
#&gt; 10 stays_in_week_nights          0 0      
#&gt; # ... with 18 more rows</code></pre>
<p>From the result above variable <code>children</code> have missing values with 4 observation, let’s fill the missing value with the 0.</p>
<pre class="r"><code>booking &lt;- booking %&gt;% 
           mutate(children = replace_na(children,0))</code></pre>
<p>Now let’s check the condition of the categorical variable using <code>inspect_cat()</code> function.</p>
<pre class="r"><code>booking %&gt;% 
  inspect_cat()</code></pre>
<pre><code>#&gt; # A tibble: 11 x 5
#&gt;    col_name               cnt common     common_pcnt levels            
#&gt;    &lt;chr&gt;                &lt;int&gt; &lt;chr&gt;            &lt;dbl&gt; &lt;named list&gt;      
#&gt;  1 arrival_date_month      12 August            11.6 &lt;tibble [12 x 3]&gt; 
#&gt;  2 assigned_room_type      12 A                 62.0 &lt;tibble [12 x 3]&gt; 
#&gt;  3 country                178 PRT               40.7 &lt;tibble [178 x 3]&gt;
#&gt;  4 customer_type            4 Transient         75.1 &lt;tibble [4 x 3]&gt;  
#&gt;  5 deposit_type             3 No Deposit        87.6 &lt;tibble [3 x 3]&gt;  
#&gt;  6 distribution_channel     5 TA/TO             82.0 &lt;tibble [5 x 3]&gt;  
#&gt;  7 hotel                    2 City Hotel        66.4 &lt;tibble [2 x 3]&gt;  
#&gt;  8 is_canceled              2 0                 63.0 &lt;tibble [2 x 3]&gt;  
#&gt;  9 market_segment           8 Online TA         47.3 &lt;tibble [8 x 3]&gt;  
#&gt; 10 meal                     5 BB                77.3 &lt;tibble [5 x 3]&gt;  
#&gt; 11 reserved_room_type      10 A                 72.0 &lt;tibble [10 x 3]&gt;</code></pre>
<p>From the result above, the country column has 178 unique value. We will reduce the unique value of the <code>country</code> to 11, namely by taking the 10 countries that appear most frequently and other countries will be changed to <strong>other</strong>.</p>
<pre class="r"><code>booking &lt;- booking %&gt;% 
  mutate(country = fct_lump_n(country, n = 10)) 

booking %&gt;% 
  inspect_cat() %&gt;% 
  show_plot()</code></pre>
<p><img src="/blog/xgboost_files/figure-html/unnamed-chunk-7-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Before we do the modeling, let’s first check the proportions of the target class to find out how balanced the target class.</p>
<pre class="r"><code>booking %&gt;% 
  pull(is_canceled) %&gt;% 
  table() %&gt;% 
  prop.table()</code></pre>
<pre><code>#&gt; .
#&gt;         0         1 
#&gt; 0.6295837 0.3704163</code></pre>
<p>Class with label 0 has a proportion of about 63% of the data while class with label 1 has a proportion of 37%, this shows that class with label 0 is more dominant.</p>
</div>
<div id="modelling" class="section level4">
<h4>Modelling</h4>
<p>We’ll create our training and testing data using <code>initial_split</code> function</p>
<pre class="r"><code>set.seed(100)
splitted &lt;- initial_split(booking, prop = 0.8,strata = is_canceled)
data_train &lt;- training(splitted)
data_test &lt;- testing(splitted)</code></pre>
<p>The function used to create the AdaBoost model is <code>adaboost()</code> from the <code>fastAdaboost</code> package. There are 3 parameters that can be filled in this function:<br />
- <code>formula</code>: Formula for models<br />
- <code>data</code>: Data used in the modeling process<br />
- <code>nIter</code>: Number of stumps used on the model</p>
<pre class="r"><code>model_ada &lt;- adaboost(formula = is_canceled~.,
                      data = data_train, 
                      nIter = 100)</code></pre>
<p>As we know each stump in the model has a different weight, the weight of each stump can be seen in <code>model_ada$weights</code>. When the weights are visualized, it will be seen that the stump formed at the end of the iteration has a smaller weight when compared to the stump formed at the beginning of the iteration.</p>
<pre class="r"><code>plot_weights &lt;- data.frame(stump_id = c(1:100), 
           weight = model_ada$weights) %&gt;% 
  ggplot(aes(y = weight, x = stump_id)) +
  geom_col(fill = &quot;dodgerblue3&quot;)
plot_weights</code></pre>
<p><img src="/blog/xgboost_files/figure-html/unnamed-chunk-13-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Now let’s predict the test dataset</p>
<pre class="r"><code>pred_hotel &lt;- predict(object = model_ada, newdata = data_test)
str(pred_hotel)</code></pre>
<pre><code>#&gt; List of 5
#&gt;  $ formula:Class &#39;formula&#39;  language is_canceled ~ .
#&gt;   .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_GlobalEnv&gt; 
#&gt;  $ votes  : num [1:23877, 1:2] 22.2 23.6 14.2 22.5 17.4 ...
#&gt;  $ class  : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 2 1 2 2 ...
#&gt;  $ prob   : num [1:23877, 1:2] 0.9 0.956 0.576 0.912 0.704 ...
#&gt;  $ error  : num 0.113</code></pre>
<p>the predicted object has several components :<br />
- $votes : Total weighted votes achieved by each class<br />
- $class : The class predicted by the classifier<br />
- $prob : A matrix with predicted probability of each class for each observation<br />
- $eror : The error on the test data if labeled. (1-accuracy)</p>
<p>Now let’s check how good our model using confusion matrix</p>
<pre class="r"><code>confusionMatrix(data = pred_hotel$class, reference = data_test$is_canceled, positive = &quot;1&quot;)</code></pre>
<pre><code>#&gt; Confusion Matrix and Statistics
#&gt; 
#&gt;           Reference
#&gt; Prediction     0     1
#&gt;          0 13725  1400
#&gt;          1  1308  7444
#&gt;                                               
#&gt;                Accuracy : 0.8866              
#&gt;                  95% CI : (0.8825, 0.8906)    
#&gt;     No Information Rate : 0.6296              
#&gt;     P-Value [Acc &gt; NIR] : &lt; 0.0000000000000002
#&gt;                                               
#&gt;                   Kappa : 0.7563              
#&gt;                                               
#&gt;  Mcnemar&#39;s Test P-Value : 0.08034             
#&gt;                                               
#&gt;             Sensitivity : 0.8417              
#&gt;             Specificity : 0.9130              
#&gt;          Pos Pred Value : 0.8505              
#&gt;          Neg Pred Value : 0.9074              
#&gt;              Prevalence : 0.3704              
#&gt;          Detection Rate : 0.3118              
#&gt;    Detection Prevalence : 0.3665              
#&gt;       Balanced Accuracy : 0.8773              
#&gt;                                               
#&gt;        &#39;Positive&#39; Class : 1                   
#&gt; </code></pre>
<p>Based on the confusion matrix above, we know that the accuracy of model is 0.88. Since we know that our data is dominated by the class labeled 0 (67%) we have to use another metrics to find out how well our model predicts the two classes. we’re going to use the AUC.</p>
<pre class="r"><code>pred_df &lt;- pred_hotel$prob %&gt;% 
  as.data.frame() %&gt;% 
  rename(class0 = V1, 
         class1 = V2) %&gt;% 
  mutate(predicted = pred_hotel$class, 
         actual = data_test$is_canceled)

auc_ada &lt;- roc_auc(data = pred_df, truth = actual,class1) 
auc_ada</code></pre>
<pre><code>#&gt; # A tibble: 1 x 3
#&gt;   .metric .estimator .estimate
#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
#&gt; 1 roc_auc binary         0.955</code></pre>
<p>The AUC results show that the model formed is good at predicting the target class, this is indicated by the AUC value of 0.95 (the closer to 1 the better).</p>
<p>AdaBoost has a lot of advantages, mainly it is easier to use with less need for tweaking parameters unlike algorithms like XGBoost. AdaBoost also can reduce the variance in testing data.</p>
</div>
</div>
</div>
<div id="xgboost" class="section level2">
<h2>XGBoost</h2>
<p>XGBoost was formulated by Tianqi Chen which started as a research project a part of <em>The Distributed Deep Machine Leaning Community (DMLC) grop</em>. XGBoost is one of popular algorithm because it has been the winning algorithm in a number of recent Kaggle competitions. XGBoost is a specific implementation of the Gradient Boosting Model which uses more accurate approximations to find the best tree model[^2]. XGBoost specifically used a more regularized model formalization to control overfitting, which gives it better perfomance.</p>
</div>
</div>
<div id="how-xgboost-works" class="section level1">
<h1>How XGBoost works?</h1>
<center>
<img src="/img/xgboost/xgboost.png" />
</center>
<p>System Optimization: [^5]</p>
<ol style="list-style-type: decimal">
<li>Parallelized tree building</li>
</ol>
<p>XGBoost approaches the process of sequential tree building using parrellelized implementation.</p>
<ol start="2" style="list-style-type: decimal">
<li>Tree pruning</li>
</ol>
<p>Unlike GBM, where tree pruning stops once a negative loss is encountered, XGBoost grows the tree up to max_depth and then prune backward until the improvement in loss function is below a threshold.</p>
<ol start="3" style="list-style-type: decimal">
<li>Cache awareness and out of core computing</li>
</ol>
<p>XGBoost has been designed to efficiently reduce computing time and allocate an optimal usage of memory resources. This is accomplished by cache awareness by allocating internal buffers in each thread to store gradient statistics. Further enhancements such as ‘out-of-core’ computing optimize available disk space while handling big data-frames that do not fit into memory.</p>
<ol start="4" style="list-style-type: decimal">
<li>Regularization</li>
</ol>
<p>The biggest advantage of xgboost is regularization. Regularization is a technique used to avoid overfitting in linear and tree based models which limits, regulates or shrink the estimated coefficient towards zero.</p>
<ol start="5" style="list-style-type: decimal">
<li>Handles missing value</li>
</ol>
<p>This algorithm has important features of handling missing values by learns the best direction for missing values. The missing values are treated them to combine a sparsity-aware split finding algorithm to handle different types of sparsity patterns in data.</p>
<ol start="6" style="list-style-type: decimal">
<li>Built-in cross validation</li>
</ol>
<p>The algorithm comes with built in cross validation method at each iteration, taking away the need to explicitly program this search and to specify the exact number of boosting iterations required in a single run.</p>
<div id="regularization-and-training-loss" class="section level2">
<h2>Regularization and training loss</h2>
<p>XGBoost offers additional regularization term controls the complexity of the model, which help us to avoid overfitting. The objective function is to measure how well the model fit the training data. They consist of two parts: training loss and the regularization term:</p>
<p><span class="math inline">\(obj(\theta )= L(\theta )+\Omega (\theta )\)</span></p>
<p>Where <span class="math inline">\(L\)</span> is <code>training loss function</code> and <span class="math inline">\(\Omega\)</span> is regularization. <code>Training loss function</code> measures how well model fit on training data, <span class="math inline">\(\Omega\)</span> will reduce the complexity of the tree functions.[^3]</p>
<p>For regression case, <code>training loss function</code> will obtain from <code>Mean Squared Error</code> value:</p>
<p><span class="math inline">\(L(\theta ) = {\sum_i^{n}(y_i-\hat{y}_i)^2}\)</span></p>
<p>Another loss function for classification case:</p>
<p><span class="math inline">\(L(\theta ) = {\sum_i[y_iln(1+e^{-\hat{y}_i})+(1-y_i)ln(1+e^{\hat{y}_i})]}\)</span></p>
</div>
<div id="case-example-using-xgboost" class="section level2">
<h2>Case Example using XGBoost</h2>
<div id="modelling-1" class="section level3">
<h3>Modelling</h3>
<pre class="r"><code>set.seed(100)
splitted &lt;- initial_split(booking, prop = 0.8,strata = is_canceled)
data_train &lt;- training(splitted)
data_test &lt;- testing(splitted)</code></pre>
<p>Split the target variable into <code>label_train</code> and <code>label_test</code></p>
<pre class="r"><code>label_train &lt;- as.numeric(as.character(data_train$is_canceled))
label_test &lt;- as.numeric(as.character(data_test$is_canceled))</code></pre>
<p>The most important thing when we work with XGBoost is converting the data to Dmatrix, because XGBoost requires a matrix input for the features.</p>
<pre class="r"><code># convert data to matrix
train_matrix &lt;- data.matrix(data_train[,-2])
test_matrix &lt;- data.matrix(data_test[,-2])
# convert data to Dmatrix
dtrain &lt;- xgb.DMatrix(data = train_matrix, label = label_train)
dtest &lt;- xgb.DMatrix(data = test_matrix, label = label_test)</code></pre>
<div id="tuning-parameters" class="section level4">
<h4>Tuning Parameters</h4>
<p>There is no benchmark to define the ideal parameters because it will depend on your data and specific problem. XGBoost Parameters can defined into three categories:[^6]</p>
<div id="general-parameters" class="section level5">
<h5>General Parameters</h5>
<p>Controls the booster type in the model which eventually drives overall functioning</p>
<ol style="list-style-type: decimal">
<li>booster</li>
</ol>
<p>For classification problems, we can use <code>gbtree</code> parameter. In <code>gbtree</code> a tree is grown one after other and attempts to reduce misclassification reate in subsequent iterations. The next tree is built by giving a higher weight to misclassified points by the previous tree.</p>
<p>For regression problems, we can use <code>gbtree</code> and <code>gblinear</code>. In <code>gblinear</code>, it builds a generalized linear model and optimizes it using regularization and gradient descent. The next model will built on residuals generated by previous iterations.</p>
<ol start="2" style="list-style-type: decimal">
<li>nthread</li>
</ol>
<p>To enable parallel computing. The default is the maximum number of cores available</p>
<ol start="3" style="list-style-type: decimal">
<li>verbosity</li>
</ol>
<p>Verbosity to display warning messages.The default value is 1 (warning), 0 for silent, 2 for info, and 3 for debug.</p>
</div>
<div id="booster-parameters" class="section level5">
<h5>Booster Parameters:</h5>
<p>Controls the performance of the selected booster</p>
<ol style="list-style-type: decimal">
<li>Eta</li>
</ol>
<p>The range of eta is 0 to 1 and default value is 0.3. It controls the maximum number of iterations, the lower eta will generate the slower computation.</p>
<ol start="2" style="list-style-type: decimal">
<li>Gamma</li>
</ol>
<p>The range of gamma is 0 to infinite and default value is 0 (no regularization). The higher gamma is the higher regularization, regularization means penalizing large coefficients that don’t improve the model’s performance.</p>
<ol start="3" style="list-style-type: decimal">
<li>nrounds</li>
</ol>
<p>it refers to controls the maximum number of iterations.</p>
<ol start="4" style="list-style-type: decimal">
<li>nfold</li>
</ol>
<p>The number of observation data is randomly partitioned into <code>nfold</code> equal size subsamples</p>
<ol start="5" style="list-style-type: decimal">
<li>max_depth</li>
</ol>
<p>Maximum depth of a tree. The range of max_depth is 0 to infinite and default value is 6, increasing this value will make the model more complex and more likely to overfit.</p>
<ol start="6" style="list-style-type: decimal">
<li>Min_child_weight</li>
</ol>
<p>The range of min_child_weight is 0 to infinite and default value is 1. If the leaf node has a minimum sum of instance weight lower than min_child_weight in the tree partition step than the process of splitting the tree will stop growing.</p>
<ol start="7" style="list-style-type: decimal">
<li>subsample</li>
</ol>
<p>The range of subsample is 0 to 1 and default value is 1. It controls the number of ratio observations to a tree. If the value is set to 0.5 means that XGboost would randomly sample half of the training data prior to growing trees and this will prevent overfitting. subsample will occur once in every boosting iteration.</p>
<ol start="8" style="list-style-type: decimal">
<li>colsampe_bytree</li>
</ol>
<p>The range of colsample_bytree is 0 to 1 and default value is 1. It controls the subsample ratio of columns when constructing each tree.</p>
</div>
<div id="learning-task-parameters" class="section level5">
<h5>Learning Task Parameters</h5>
<p>Sets and evaluates the learning process of booster from the given data.</p>
<ol style="list-style-type: decimal">
<li>Objective</li>
</ol>
<ul>
<li><code>reg:squarederror</code> for regression with squared loss</li>
<li><code>binary:logistic</code> for binary classification</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>eval_metric</li>
</ol>
<p>Evaluation metrics for validaton data. The default is RMSE for regression case and error for classification case.</p>
<p>Next, we define the parameter will be used:</p>
<pre class="r"><code>params &lt;- list(booster = &quot;gbtree&quot;,
               objective = &quot;binary:logistic&quot;,
               eta=0.1, 
               gamma=10, 
               max_depth=10, 
               min_child_weight=1, 
               subsample=1, 
               colsample_bytree=1)</code></pre>
<p>One of the simplest way to see the training progress is to set the verbose option as <code>TRUE</code>.</p>
<pre class="r"><code>tic()
xgbcv &lt;- xgb.cv( params = params, 
                 data = dtrain,
                 nrounds = 1000, 
                 showsd = T, 
                 nfold = 5,
                 stratified = T, 
                 print_every_n = 50, 
                 early_stopping_rounds = 20, 
                 maximize = F)</code></pre>
<pre><code>#&gt; [1]  train-error:0.159371+0.000575   test-error:0.162041+0.002767 
#&gt; Multiple eval metrics are present. Will use test_error for early stopping.
#&gt; Will train until test_error hasn&#39;t improved in 20 rounds.
#&gt; 
#&gt; [51] train-error:0.134139+0.000721   test-error:0.140127+0.002438 
#&gt; [101]    train-error:0.122489+0.000533   test-error:0.132233+0.001973 
#&gt; Stopping. Best iteration:
#&gt; [114]    train-error:0.121790+0.000932   test-error:0.131793+0.002073</code></pre>
<pre class="r"><code>print(xgbcv)</code></pre>
<pre><code>#&gt; ##### xgb.cv 5-folds
#&gt;     iter train_error_mean train_error_std test_error_mean test_error_std
#&gt;        1        0.1593710    0.0005746874       0.1620406    0.002766615
#&gt;        2        0.1589574    0.0008488257       0.1616114    0.002560948
#&gt;        3        0.1582874    0.0008635417       0.1612242    0.002240224
#&gt;        4        0.1574078    0.0012001357       0.1600936    0.002275888
#&gt;        5        0.1563270    0.0017212650       0.1593920    0.001805679
#&gt; ---                                                                     
#&gt;      130        0.1217530    0.0009838978       0.1318562    0.002019269
#&gt;      131        0.1217504    0.0009883673       0.1318562    0.002019269
#&gt;      132        0.1217504    0.0009883673       0.1318562    0.002019269
#&gt;      133        0.1217504    0.0009883673       0.1318562    0.002019269
#&gt;      134        0.1217504    0.0009883673       0.1318562    0.002019269
#&gt; Best iteration:
#&gt;  iter train_error_mean train_error_std test_error_mean test_error_std
#&gt;   114        0.1217898    0.0009316736       0.1317934    0.002072701</code></pre>
<pre class="r"><code>toc()</code></pre>
<pre><code>#&gt; 72.96 sec elapsed</code></pre>
<pre class="r"><code>tic()
xgb1 &lt;- xgb.train (params = params, 
                   data = dtrain, 
                   nrounds = xgbcv$best_iteration, 
                   watchlist = list(val=dtest,train=dtrain),
                   print_every_n = 100, 
                   early_stoping_rounds = 10, 
                   maximize = F , 
                   eval_metric = &quot;error&quot;,
                   verbosity = 0)</code></pre>
<pre><code>#&gt; [1]  val-error:0.157641  train-error:0.158900 
#&gt; [101]    val-error:0.125644  train-error:0.121533 
#&gt; [114]    val-error:0.124011  train-error:0.119858</code></pre>
<pre class="r"><code>toc()</code></pre>
<pre><code>#&gt; 15.86 sec elapsed</code></pre>
<pre class="r"><code>xgbpred_prob &lt;-predict(object = xgb1, newdata = dtest)
xgbpred &lt;- ifelse (xgbpred_prob &gt; 0.5,1,0)</code></pre>
<p>In this section, we evaluate the performance of XGBoost model</p>
<pre class="r"><code>confusionMatrix(as.factor(xgbpred), as.factor(label_test))</code></pre>
<pre><code>#&gt; Confusion Matrix and Statistics
#&gt; 
#&gt;           Reference
#&gt; Prediction     0     1
#&gt;          0 13902  1830
#&gt;          1  1131  7014
#&gt;                                                
#&gt;                Accuracy : 0.876                
#&gt;                  95% CI : (0.8717, 0.8801)     
#&gt;     No Information Rate : 0.6296               
#&gt;     P-Value [Acc &gt; NIR] : &lt; 0.00000000000000022
#&gt;                                                
#&gt;                   Kappa : 0.7297               
#&gt;                                                
#&gt;  Mcnemar&#39;s Test P-Value : &lt; 0.00000000000000022
#&gt;                                                
#&gt;             Sensitivity : 0.9248               
#&gt;             Specificity : 0.7931               
#&gt;          Pos Pred Value : 0.8837               
#&gt;          Neg Pred Value : 0.8611               
#&gt;              Prevalence : 0.6296               
#&gt;          Detection Rate : 0.5822               
#&gt;    Detection Prevalence : 0.6589               
#&gt;       Balanced Accuracy : 0.8589               
#&gt;                                                
#&gt;        &#39;Positive&#39; Class : 0                    
#&gt; </code></pre>
<p>let’s check the variable importance from the model:</p>
<pre class="r"><code>var_imp &lt;- xgb.importance(model = xgb1,
                          feature_names = dimnames(dtrain)[[2]])
var_imp %&gt;% 
  mutate_if(is.numeric, round, digits = 2)</code></pre>
<pre><code>#&gt;                            Feature Gain Cover Frequency
#&gt;  1:                   deposit_type 0.35  0.07      0.01
#&gt;  2:                        country 0.13  0.12      0.10
#&gt;  3:                      lead_time 0.09  0.11      0.13
#&gt;  4:                 market_segment 0.07  0.06      0.04
#&gt;  5:      total_of_special_requests 0.06  0.06      0.03
#&gt;  6:    required_car_parking_spaces 0.05  0.08      0.02
#&gt;  7:         previous_cancellations 0.04  0.05      0.03
#&gt;  8:              arrival_date_year 0.04  0.04      0.06
#&gt;  9:                            adr 0.03  0.10      0.14
#&gt; 10:                  customer_type 0.02  0.03      0.04
#&gt; 11:       arrival_date_week_number 0.02  0.04      0.09
#&gt; 12:             reserved_room_type 0.02  0.03      0.03
#&gt; 13:                booking_changes 0.01  0.03      0.03
#&gt; 14:             assigned_room_type 0.01  0.05      0.03
#&gt; 15: previous_bookings_not_canceled 0.01  0.02      0.02
#&gt; 16:                          hotel 0.01  0.02      0.02
#&gt; 17:           stays_in_week_nights 0.01  0.02      0.03
#&gt; 18:             arrival_date_month 0.01  0.01      0.02
#&gt; 19:      arrival_date_day_of_month 0.01  0.01      0.03
#&gt; 20:        stays_in_weekend_nights 0.01  0.01      0.02
#&gt; 21:                           meal 0.01  0.02      0.02
#&gt; 22:           distribution_channel 0.00  0.01      0.01
#&gt; 23:                         adults 0.00  0.01      0.02
#&gt; 24:                       children 0.00  0.01      0.01
#&gt; 25:              is_repeated_guest 0.00  0.01      0.01
#&gt; 26:           days_in_waiting_list 0.00  0.01      0.01
#&gt; 27:                         babies 0.00  0.00      0.00
#&gt;                            Feature Gain Cover Frequency</code></pre>
<p>The function of <code>xgb.importance</code>displays the result importance values calculated with different importance metrics:</p>
<ul>
<li><p>The <strong>gain</strong> value means the percentage contribution of the feature for each tree in the model</p></li>
<li><p>The <strong>cover</strong> value means the percentage represents the number of observations for each feature from all trees. From example, if we have 100 observations with 3 tree and each tree have 5, 8, and 10 observations for feature “A”. The cover value will calculate 5+8+10 = 23 observations from all trees for each feature. In this case, the feature “A” has a 0.23 cover value.</p></li>
<li><p>The <strong>frequency</strong> value means the percentage representing the number of times a feature will splits in the trees of the model. For example, feature “A” occurred in 3 splits, 2 splits, and 2 splits for each tree. So the value of frequency feature “A” is 3+2+2=7 splits divide with all total numbers splits for all features.</p></li>
</ul>
<pre class="r"><code>xgb.ggplot.importance(var_imp,top_n = 10) + theme_minimal()</code></pre>
<p><img src="/blog/xgboost_files/figure-html/unnamed-chunk-26-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The graph shows the variable importance used the gain value by default and it also displays the cluster of features that have similar feature importances value. From 10 above features means their features have a significant impact on the result of prediction.</p>
<p>Next, we evaluate the perfomance model on the ROC curver</p>
<pre class="r"><code>xgb_result &lt;- data.frame(class1 = xgbpred_prob, actual = as.factor(label_test))

auc_xgb &lt;- roc_auc(data = xgb_result, truth = actual,class1) 

result &lt;- rbind(auc_ada, auc_xgb) %&gt;% 
          mutate(model = c(&quot;AdaBoost&quot;, &quot;XGBoost&quot;)) %&gt;% 
          select(model, everything())
result</code></pre>
<pre><code>#&gt; # A tibble: 2 x 4
#&gt;   model    .metric .estimator .estimate
#&gt;   &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
#&gt; 1 AdaBoost roc_auc binary         0.955
#&gt; 2 XGBoost  roc_auc binary         0.948</code></pre>
<p>The AUC results show that AdaBoost and XGBoost model have similar value 0.94 and 0.95. To obtain the AdaBoost model we need to run model for 60 minutes, while the XGBoost model only need ~60 seconds. We can say that XGBoost works better than AdaBoost for speed.</p>
</div>
</div>
</div>
<div id="conclusion" class="section level3">
<h3>Conclusion</h3>
<p>In this article, we described the lesson on how to building and how AdaBoost and XGBoost model works. We can conclude several points:</p>
<ul>
<li><p>Both of two algorithms are built based on converting weak learners to a strong learner</p></li>
<li><p>AdaBoost has only a few hyperparameters to improve the model but this model is easy to understand and to visualize</p></li>
<li><p>The decision which algorithm will be used depends on our data set, for low noise data and timeliness of result is not the main concern, we can use AdaBoost model</p></li>
<li><p>For complexity and high dimension data, XGBoost performs works better than Adaboost because XGBoost have system optimizations.</p></li>
</ul>
</div>
</div>
</div>
<div id="reference" class="section level1">
<h1>Reference</h1>
<p>[^1] : <a href="https://www.kdnuggets.com/2017/10/xgboost-top-machine-learning-method-kaggle-explained.html">XGBoost, a Top Machine Learning Method on Kaggle</a></p>
<p>[^2] : <a href="https://towardsdatascience.com/xgboost-the-excalibur-for-everyone-8009bd015f1e">XGBoost: The Excalibur for Everyone</a></p>
<p>[^3] : <a href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html#">Introduction to Boosted Trees</a></p>
<p>[^4] : <a href="https://www.shirin-glander.de/2018/11/ml_basics_gbm/">Machine Learning Basics-Gradient Boosting &amp; XGBoost</a></p>
<p>[^5] : <a href="source:%20https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d">XGBoost Algorithm</a></p>
<p>[^6] : <a href="https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/beginners-tutorial-on-xgboost-parameter-tuning-r/tutorial/">Parameter Tuning in R</a></p>
</div>
