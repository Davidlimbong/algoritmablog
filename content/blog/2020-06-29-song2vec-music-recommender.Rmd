---
title: "Song2Vec: Music Recommender"
author: "Tomy Tjandra"
date: '2020-06-29'
github: https://github.com/tomytjandra
slug: song2vec-music-recommender
categories:
  - Python
tags:
  - Machine Learning
  - Topic Modelling
  - Recommender System
  - gensim
description: ''
featured: ''
featuredalt: ''
featuredpath: ''
linktitle: ''
type: post
---

<style>
body {
text-align: justify}
</style>

```{r, echo=FALSE}
rm(list = ls())

knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  fig.align = "center",
  comment = "#>"
)
```

```{r, include=F}
Sys.setenv(RETICULATE_PYTHON = "C:/Users/tomyt/anaconda3/envs/word-embeddings/python.exe")
library(reticulate)

py_run_string("import os")
py_run_string("os.environ['QT_QPA_PLATFORM_PLUGIN_PATH'] = 'C:/Users/tomyt/anaconda3/Library/plugins/platforms'")
```

# Background

The behavior of musicophiles has changed along with the evolvement of the music industry in the past decades. Previously we conservatively bought music on a compact disc, but now music streaming services are more preferable; such as Amazon Music,
Apple Music, Google Play Music, Pandora, Spotify, Youtube Music, to name a few. This is because of the convenience offered by these platforms so that users are able to search their favorite songs right away without having to bother going to the music store physically.

Users may not have enough time to scan through all available songs and manually create a playlist. Instead, a recommender system is constructed which eases them to find relevant songs quickly. One example you might seen before is the "Made For You" feature from Spotify:

<center> ![](/img/song2vec-music-recommender/made-for-you.png){width="80%"} </center>

These personalized playlists are being recommended by grouping similar songs that go well together. How? In the real case, this process is done by combining several recommender algorithms, simply based on users' activities such as likes, playlist history, or even listening history. In this article, we will demonstrate how to extract song embeddings using a neural network approach specifically **Word2Vec** model, use it to generate songs recommendation, and evaluate the performance.

Okay, so you might wonder what is Word2Vec actually? Developed by Tomas Mikolov [^1] in 2013 at Google, it is one of the most common technique to do word embeddings in several Natural Language Processing (NLP) cases using shallow neural network. Word embeddings is just a fancy way of saying a numerical representation of words. A good analogy would be how colors are represented with a RGB values. These set of values is then called as a **vector**. For example, "black" can be associated with (0,0,0) and "white" with (255,255,255) as their pixel intensity values.

```{python, echo=F, results='hide', out.width = '90%'}
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

plt.figure(figsize=(10, 6))
ax = plt.subplot(111, projection='3d')

colors = [
    (255,0,0),
    (0,255,0),
    (0,0,255),
    (225,225,225)
]

for r, g, b in colors:
    ax.quiver(0, 0, 0, r, g, b,
              color = np.array([r, g, b])/255,
              linewidth = 5)

ax.set_xlim3d(0, 255)
ax.set_ylim3d(0, 255)
ax.set_zlim3d(0, 255)
ax.dist = 11
plt.title("COLOR VECTORS", fontweight = "bold")
plt.tight_layout()
plt.show()
```

In fact, word embeddings method can be generalized into other item embeddings, which associate any product on an e-commerce website, any videos on Youtube, movies on Netflix with a vector. Of course, in this case, songs can also be a vector.

Can you guess what is the property of a sentence that Word2Vec exploits to learn the vector representation of a word? It is the **sequential nature of the text**. Take a look of the following scrambled sentence:

> *gives Spotify millions you access music service to digital a that is of songs.*

It is difficult for us to understand the text because there is no sequence in the sentence. That's why the sequence of words is crucial in any natural language. This property can be implemented to other data that has sequential nature as well. One such data that has the property is **playlist of songs** in music streaming services. The following image is an example of playlists in Spotify, where each playlist contain a sequence of songs:

<center> ![](/img/song2vec-music-recommender/playlist-menu.png){width="80%"} </center>

Since the data cleansing and modelling process will be quite complicated, here I present the visualization for you to understand the overall workflow for this article:

<center> ![](/img/song2vec-music-recommender/workflow.png){width="100%"} </center>

# Import Libraries

Before going any further, let's import necessary libraries such as:

- `pandas` for data analysis
- `numpy` for scientific computing
- `matplotlib` and `seaborn` for data visualization
- `gensim` for topic modelling, in this case Word2vec
- `sklearn` and `spherecluster` for other unsupervised learning algorithm

```{python}
# Data Analysis
import pandas as pd
import numpy as np

# Visualization
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import seaborn as sns
plt.style.use('seaborn')
sns.set_style("whitegrid")

# Modelling
from gensim.models import Word2Vec
from gensim.models.callbacks import CallbackAny2Vec
from spherecluster import SphericalKMeans
from sklearn.model_selection import train_test_split
from sklearn.manifold import TSNE
from scipy import stats

# Additional
import math
import random
import itertools
import multiprocessing
from tqdm import tqdm
from time import time
import logging
import pickle
```

# Data Cleansing

Human-made music playlists collected by Shuo Chen [^2] from Cornell University are used to learn the song embeddings. The dataset contains US radio playlists from Yes.com and songs tag from Last.fm since December 2010 to May 2011. Each playlist will be treated as a sentence and each song in the playlist will be treated as one word.

<center> ![](/img/song2vec-music-recommender/list-illustration.png){width="80%"} </center>

The raw data consists of five separate txt files as follow:

1. `song_hash.txt`: mapping from integer `song_id` to song's `title` and `artist` name
2. `tags.txt`: social tags, using integer `song_id` to represent a song
3. `tag_hash.txt`: mapping from integer id to tag's name
4. `train.txt` and `test.txt`: playlists using integer `song_id` to represent a song

```{python include=F}
FOLDER_PATH = "song2vec-music-recommender/dataset/yes_complete/"
```

## Songs
Each song has its own `song_id` which maps to exactly one `title` and `artist` name. There are 75252 unique songs present in `song_hash.txt`.

```{python}
songs = pd.read_csv(FOLDER_PATH+"song_hash.txt", sep = '\t', header = None,
                    names = ['song_id', 'title', 'artist'], index_col = 0)
songs['artist - title'] = songs['artist'] + " - " + songs['title']
songs
```

## Tags
Each song has several tags that exist in `tags.txt` and the mapping is provided in `tag_hash.txt`.

```{python}
def readTXT(filename, start_line=0, sep=None):
    with open(FOLDER_PATH+filename) as file:
        return [line.rstrip().split(sep) for line in file.readlines()[start_line:]]
```

```{python}
tags = readTXT("tags.txt")
tags[7:12]
```

If a song does not have any tag, it is indicated with just a '#' as seen above. Replace it with the string "unknown" instead.

```{python}
mapping_tags = dict(readTXT("tag_hash.txt", sep = ', '))
mapping_tags['#'] = "unknown"
```

The `song_tags` dataframe is combined and merged with previous `songs`.

```{python}
song_tags = pd.DataFrame({'tag_names': [list(map(lambda x: mapping_tags.get(x), t)) for t in tags]})
song_tags.index.name = 'song_id'
songs = pd.merge(left = songs, right = song_tags, how = 'left',
                 left_index = True, right_index = True)
songs.index = songs.index.astype('str')
songs.head()
```

## Unknown Songs

Unknown songs are defined as a song that doesn't have either `artist` or `title`, indicated by dash (-) character. Remove these unknown songs from `songs`.

```{python}
unknown_songs = songs[(songs['artist'] == '-') | (songs['title'] == '-')]
songs.drop(unknown_songs.index, inplace = True)
```

## Playlist

`playlist` is a list of lists of songs (represented with its `song_id`) from `train.txt` and `test.txt`. There are 15910 playlists that exist in the data.

```{python}
playlist = readTXT("train.txt", start_line = 2) + readTXT("test.txt", start_line = 2)
print(f"Playlist Count: {len(playlist)}")
```

Take a look at how the playlist is represented in a list. Recall that these playlists are treated as sentences and the `song_id` as a token of words.

```{python}
for i in range(0, 2):
    print("-------------------------")
    print(f"Playlist Idx. {i}: {len(playlist[i])} Songs")
    print("-------------------------")
    print(playlist[i])
```

Remove unknown songs from the playlist.

```{python}
playlist_wo_unknown = [[song_id for song_id in p if song_id not in unknown_songs.index]
                       for p in playlist]
```

Remove playlist with zero or one song, since the model wouldn't capture any sequence in that list.

```{python}
clean_playlist = [p for p in playlist_wo_unknown if len(p) > 1]
print(f"Playlist Count After Cleansing: {len(clean_playlist)}")
```

Remove song that doesn't exist in any playlist.

```{python}
unique_songs = set(itertools.chain.from_iterable(clean_playlist))
song_id_not_exist = set(songs.index) - unique_songs
songs.drop(song_id_not_exist, inplace = True)
print(f"Unique Songs After Cleansing: {songs.shape[0]}")
```

Before there are 75262 unique songs and 15910 playlists. Now we are ready with 73448 unique songs and 15842 playlists.

# Modelling

The `playlist` is splitted into `playlist_train` and `playlist_test` with test size of 1000 playlist for further evaluation.

```{python include=F}
MODEL_PATH = "song2vec-music-recommender/model/"
```

```{python}
playlist_train, playlist_test = train_test_split(clean_playlist, test_size = 1000,
                                                 shuffle = True, random_state = 123)
```

## Song2Vec

As mentioned before, Word2Vec is one of the most popular techniques to learn word embeddings using **shallow neural network**. A neural network, like other supervised learning algorithms, requires labeled data to be trained. How can we train a neural network if the data is in a form of sequences of words (i.e. words) or sequences of songs (i.e. playlist) without any target or data label? The network will be trained by creating a so-called **“fake” task**. We won't be interested in the inputs and outputs of the network, rather just train the **weights between input and hidden layer** that are extracted as the vectors. To put it in simple terms, the goal of embeddings can be classified as unsupervised learning, but the process of getting the embeddings in Word2Vec is achieved by supervised learning through a neural network.

Here is the illustration of general Word2Vec architecture by Xin Rong [^3]:

<center> ![](/img/song2vec-music-recommender/word2vec-architecture.png){width="80%"} </center>

Details:

- The input layer is a one-hot-encoded vector of size $V$ (vocabulary size).
- $W_{V \times N}$ is the weight matrix that projects the input $x$ to the hidden layer. **These values are the embedded vectors**.
- The hidden layer contains $N$ neurons (hyperparameter), it just copies the weighted sum of inputs to the next layer without any activation function.
- $W'_{N \times V}$ is the weight matrix that maps the hidden layer outputs to the final output layer.
- The output layer is again a $V$ length vector, with a softmax activation function.

There are two approaches of Word2Vec in which both are using the same architecture:

- Skip-gram - the fake task would be: given a target word, the model is trying to predict the context words.
- Continuous Bag-Of-Words (CBOW) - the fake task would be: predict the target word by the context words.

In this article, **CBOW** is used instead of Skip-gram, because according to Google Code Archive [^4], it trains faster and able to capture the frequent songs more.

<center> ![](/img/song2vec-music-recommender/playlist-example.png){width="80%"} </center>

Target song that are played between context songs is assumed to be similar to each other. If the playlist are designed by users or the services for certain genre, the song embeddings will logically incorporate more information about the genre.

One epoch of CBOW may be breakdown into these steps [^3]:

<center> ![](/img/song2vec-music-recommender/cbow.png){width="60%"} </center>

1. Convert the generated training samples into one-hot vectors $x_1, x_2, ..., x_C$ (contexts) for the input layer. So, the size is $C \times V$
2. Multiply all vector $x$ with $W_{V \times N}$ and then take the sum or mean of embedded vectors.
3. The hidden layer is then multiplied with $W'_{N \times V}$ to get the weighted sum of size $V$.
4. Apply softmax function to turn the weighted sum into probabilities, usually denoted by $\hat{y}$.
5. Error between output and each context word is calculated as follows: ${(\hat{y} - y)}$
6. Backpropagate to re-adjust the weights, by using Gradient Descent optimizer. <br>
    a. All weights in output matrix will be updated. <br>
    b. Only corresponding word vector in the input matrix that will be updated.

Up until this point, you should have understood the general overview of how the Word2Vec works. But, there is an issue with the softmax function — it is **computationally very expensive**, as it requires scanning through the entire output embeddings matrix to compute the probability distribution of all V words, where V can be millions or more. The softmax function is defined as:

<center>$softmax(y_i) = \dfrac{e^{y_i}}{\sum \limits_{y=1}^V e^{y_j}}$</center>

The normalization factor in the denominator also requires $V$ iterations. When implemented in codes, the normalization factor is computed only once and cached as a Python variable, making the algorithm complexity $O(V)$.

Due to this computational inefficiency, softmax function is preferably not used in most implementations of Word2Vec. Instead let's use an alternative called **negative sampling** with sigmoid function, which rephrases the problem into a set of independent binary logistic classification task of algorithm complexity = $O(K+1)$, where $K$ is the number of negative samples and $1$ is the positive sample. Mikolov suggests using $K$ in the range $[5, 20]$ for small vocabulary and $[2, 5]$ for a larger vocabulary.

Don't worry about the code below. We are setting up the logging settings to monitor the training process.

```{python}
logging.basicConfig(format="%(asctime)s : %(levelname)s : %(message)s", level=logging.INFO)

class Callback(CallbackAny2Vec):
    def __init__(self):
        self.epoch = 1
        self.training_loss = []

    def on_epoch_end(self, model):
        loss = model.get_latest_training_loss()
        if self.epoch == 1:
            current_loss = loss
        else:
            current_loss = loss - self.loss_previous_step
        print(f"Loss after epoch {self.epoch}: {current_loss}")
        self.training_loss.append(current_loss)
        self.epoch += 1
        self.loss_previous_step = loss
```

### Training

By using `gensim`, the training process can be separated into **three distinctive steps** [^5]: 

First, the instance of `Word2Vec()` is created to set up the parameters of the model and leave the model uninitialized.

- `size`: dimensionality of the song vectors
- `window`: maximum distance between context and target
- `min_count`: frequency cut-off for a song to be considered in the model
- `sg = 0`: using CBOW architecture
- `negative`: negative sampling data
- `workers`: number of CPU used to train the model

```{python}
model = Word2Vec(
    size = 256,
    window = 10,
    min_count = 1,
    sg = 0,
    negative = 20,
    workers = multiprocessing.cpu_count()-1)
print(model)
```

Secondly, the method `.build_vocab()` is called to build the vocabulary from a sequence of playlists and thus initialized the model.

```{python}
logging.disable(logging.NOTSET) # enable logging
t = time()

model.build_vocab(playlist_train)
```

Finally, `.train()` trains the model. The loggings here are mainly useful for monitoring the loss after each epoch.

- `total_examples`: count of unique vocabulary (songs)
- `epochs`: number of iterations over the dataset (whole playlist)
- `compute_loss`: track model loss

```{python, eval=F}
logging.disable(logging.INFO) # disable logging
callback = Callback() # instead, print out loss for each epoch
t = time()

model.train(playlist_train,
            total_examples = model.corpus_count,
            epochs = 100,
            compute_loss = True,
            callbacks = [callback]) 

model.save(MODEL_PATH+"song2vec.model")
```

```{python, echo=F}
logging.disable(logging.INFO) # disable logging
model = Word2Vec.load(MODEL_PATH+"song2vec.model")
```

```{python}
print(model)
```

### Loss Evaluation

Plot the training loss, making sure it decreases after each epoch. The closer the loss to a zero value, the better the model is in predicting a target song given surrounding context songs. Thus, the produced song vectors are more meaningful.

```{python, echo=F, results='hide', out.width = '90%'}
plt.plot(range(1, model.epochs+1), model.callbacks[0].training_loss)
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training Loss", fontweight = "bold", fontsize = 20)
plt.tight_layout()
plt.show()
```

### Vectors Visualization

The song vectors can be visualized using a gradient of colors. The model is trained using 256 dimensions, therefore there will be 256 color bars for each song, representing element values in the vector. The similarity between songs is calculated using **cosine similarity**:

<center> $similarity(A,B) = cos(\theta) = \frac{A.B}{\|A\| \|B\|}$ </center>

Mathematically it measures the cosine of the angle between two vectors $A$ and $B$ which projected in a multi-dimensional space. Song vectors with similar context occupy close spatial positions; the cosine between such vectors should be close to 1, i.e. angle is closer to 0. The smaller the angle, the cosine similarity will be higher.

```{python, echo=F, results='hide', out.width = '120%'}
fig, axes = plt.subplots(6, 1, figsize = (50, 30))

slug = '4162'
song_id_list = [(slug, "Main Song"), *[t for t in model.wv.most_similar(slug)[:5]]] 

for ax, (song_id, sim) in zip(axes.flat, song_id_list):
    ax.imshow([model.wv[song_id]], cmap = "binary", aspect = "auto")
    ax.set_title(songs.loc[song_id, "artist - title"], fontsize = 75)
    ax.yaxis.set_label_position("right")
    ax.set_ylabel(f"Similarity:\n{sim:.3f}" if sim != song_id_list[0][1] else sim,
                  rotation = "horizontal", ha = "left", va = "center", fontsize = 60)
    ax.set_xticks([])
    ax.set_yticks([])
plt.tight_layout()    
plt.show()
```

The plot above shows five most similar songs to `song_id = '4162'` (Maroon 5 - She Will Be Loved). Up until now, the model can be used for recommending new songs using cosine similarity, but only based on one main song.

## Clustering

What can we do with the song vectors? One thing is to group them into several clusters using K-Means clustering, but keep in mind that the similarity between vectors is calculated using cosine distance instead of regular (Euclidean) distance. Therefore K-Means with cosine distance should be considered, which often called **Spherical K-Means Clustering**. The idea is to identify the centroid such that it uniforms and minimizes the angle between each vector in a cluster. The intuition is just like looking at a cluster of stars where each point should have consistent spacing between each other. This spacing is referred to as the cosine similarity.

### Spherical K-Means

Take a look at the picture below for the illustration, and here are the steps:

1. Generate random 2D vectors ranging from (0,0) to (1,1).
2. Project each vector onto a unit circle, so that the vectors are normalized (length is equal to one).
3. From the projected vectors, perform basic k-means clustering into k clusters such that the vector within the same cluster are as similar as possible while the vector from different clusters is as dissimilar as possible.
4. Assign cluster number for each vector.

```{python, echo=F, results='hide', out.width = '80%'}
# Generate random vectors
np.random.seed(123)
random_vec = np.random.uniform(0, 1, (2500, 2))
skm_test = SphericalKMeans(n_clusters = 8,
                           random_state = 123).fit(random_vec)

normalized_random_vec = random_vec/np.linalg.norm(random_vec, axis=1, keepdims=True)
cluster_df = pd.DataFrame({'x': random_vec[:,0],
                           'y': random_vec[:,1],
                           'x_proj': normalized_random_vec[:,0],
                           'y_proj': normalized_random_vec[:,1],
                           'cluster': skm_test.labels_})

# Visualization
fig, axes = plt.subplots(2, 2, figsize=(10,10))

plot_list = [('x', 'y'), ('x_proj', 'y_proj'), ('x_proj', 'y_proj'), ('x', 'y')]
hue_list = [None, None, 'cluster', 'cluster']
title_list = ["1. RAW VECTOR DATA", "2. PROJECTED VECTOR ONTO A UNIT CIRCLE",
              "3. CLUSTERED PROJECTED VECTOR", "4. CLUSTERED VECTOR DATA"]

for ax, (x,y), hue, title in zip(axes.flat, plot_list, hue_list, title_list):
    sns.scatterplot(data = cluster_df,
                    x = x, y = y, hue = hue,
                    palette = "Reds", legend = False,
                    ax = ax)
    ax.set_title(title, fontweight = "bold")

plt.tight_layout()
plt.show()
```

That being said, let's perform Spherical K-Means on the song vectors by iterating the number of clusters from 10 to 500 so that the optimal number of clusters `k_opt` can be chosen by the elbow method.

```{python}
embedding_matrix = model.wv[model.wv.vocab.keys()]
embedding_matrix.shape
```

```{python, eval=F, include=F}
range_k_clusters = (10, 500)
skm_list = []
for k in tqdm(range(*range_k_clusters, 10)):
    skm = SphericalKMeans(n_clusters = k,
                          n_init = 5, n_jobs = -1,
                          random_state = 123).fit(embedding_matrix)
    
    result_dict = {
        "k": k,
        "WCSS": skm.inertia_,
        "skm_object": skm
    }
    
    skm_list.append(result_dict)
skm_df = pd.DataFrame(skm_list).set_index('k')
skm_df.head()
```

```{python, include=F}
def save2Pickle(obj, filename):
    with open(f"{MODEL_PATH}{filename}.pkl", "wb") as file:
        pickle.dump(obj, file)

def loadPickle(filename):
    with open(f"{MODEL_PATH}{filename}.pkl", "rb") as file:
        return pickle.load(file)
```

```{python, echo=F, results='hide', out.width = '80%'}
skm_df = loadPickle("skm_cluster")
plt.figure(figsize=(15, 8))
plt.plot(skm_df.WCSS)
plt.xlabel("No. of Clusters", fontsize = 20)
plt.ylabel("Within Cluster Sum of Squares", fontsize = 20)
plt.title("Elbow Method", fontweight = "bold", fontsize = 25)
plt.tight_layout()
plt.show()
```

How to locate the optimal number of clusters objectively? Here is the idea [^6]:

<center> ![](/img/song2vec-music-recommender/elbow-method.png){width="60%"} </center>

1. Connect the first and last point of the curve with a straight line
2. Calculate the perpendicular distance from each point to that line
3. Consider the longest distance as the elbow

```{python, include=F}
def locateOptimalElbow(x, y):
    # START AND FINAL POINTS
    p1 = (x[0], y[0])
    p2 = (x[-1], y[-1])
    
    # EQUATION OF LINE: y = mx + c
    m = (p2[1] - p1[1]) / (p2[0] - p1[0])
    c = (p2[1] - (m * p2[0]))
    
    # DISTANCE FROM EACH POINTS TO LINE mx - y + c = 0
    a, b = m, -1
    dist = np.array([abs(a*x0+b*y0+c)/math.sqrt(a**2+b**2) for x0, y0 in zip(x,y)])
    return x[np.argmax(dist)]
```

```{python}
k_opt = locateOptimalElbow(skm_df.index, skm_df['WCSS'].values)
skm_opt = skm_df.loc[k_opt, "skm_object"]
skm_opt
```

```{python}
songs_cluster = songs.copy()
songs_cluster.loc[model.wv.vocab.keys(), 'cluster'] = skm_opt.labels_
songs_cluster['cluster'] = songs_cluster['cluster'].fillna(-1).astype('int').astype('category')
songs_cluster.head()
```

In the end, the optimal number of clusters is set to be 110. There is a possibility that some songs don't have the embedded vectors since the `playlist` is split to train and test. For this case, assign the cluster as -1 instead.

### Visualize Clusters

It is always quite helpful to visualize the embeddings that have been created. Over here, we have song vectors with 256 dimensions. These high-dimensional vectors can't be visualized in our 3D world, so using dimensionality reduction algorithms such as **t-Distributed Stochastic Neighbor Embedding (t-SNE)** helps us map the vectors to a lower dimension. The mathematical detail of t-SNE will not be presented here, but in practice, it tends to produce a visualization with distinctly isolated clusters.

```{python, eval=F}
embedding_tsne = TSNE(n_components = 2, metric = 'cosine',
                      random_state = 123).fit_transform(embedding_matrix)
                      
save2Pickle(embedding_tsne, "tsne_viz")
```

```{python, echo=F, results='hide', out.width = '80%'}
embedding_tsne = loadPickle("tsne_viz")
songs_cluster.loc[model.wv.vocab.keys(), 'x'] = embedding_tsne[:,0]
songs_cluster.loc[model.wv.vocab.keys(), 'y'] = embedding_tsne[:,1]

plt.figure()
sns.scatterplot(data = songs_cluster[songs_cluster['cluster'] != -1],
                x = 'x', y = 'y', palette = "viridis",
                hue = 'cluster', legend = False).set_title(f"{k_opt} Clusters of Song2Vec",
                                                           fontweight = "bold", fontsize = 15)
plt.show()
```

The cluster might look cluttered since all 110 clusters are being plotted at once. Instead, let's just perform t-SNE on randomly selected 10 clusters and visualize the result.

```{python, echo=F, results='hide', out.width = '80%'}
random.seed(100)
random_cluster2plot = random.sample(range(k_opt), 10)
random_songs = songs_cluster[songs_cluster.cluster.isin(random_cluster2plot)].copy()
random_tsne = TSNE(n_components = 2, metric = 'cosine',
                   random_state = 100).fit_transform(model.wv[random_songs.index])
random_songs.loc[random_songs.index, 'x'] = random_tsne[:,0]
random_songs.loc[random_songs.index, 'y'] = random_tsne[:,1]

plt.figure()
g = sns.scatterplot(data = random_songs,
                x = 'x', y = 'y', palette = "viridis",
                hue = 'cluster', legend = False)
g.set_title(f"Randomly selected {len(random_cluster2plot)} clusters of Song2Vec", fontweight = "bold", fontsize = 15)
plt.show()
```

Songs that have similar context (by cosine similarity) tend to be plotted next to each other. Thus, creating distinct song clusters. Note that the clusters might look overlap to each other due to the dimensionality reduction, but in the actual dimension, they do not.

# Start Recommending

Congratulations! We are finally ready with the embeddings for every song that exists in `playlist_train`. How these song vectors are then used to suggest similar songs based on a certain playlist? One way is to calculate a **playlist vector** for each playlist by averaging together all the song vectors in that playlist. These vectors then become the query to find similar songs based on cosine similarity. Here is an illustration using a users' music streaming playlist [^7]:

<center> ![](/img/song2vec-music-recommender/playlist-vector.gif){width="80%"} </center>

For each playlist in `playlist_test`, calculate the average vectors using `meanVectors()` function. If the song hasn't been embedded before, neglect the song instead.

```{python}
def meanVectors(playlist):
    vec = []
    for song_id in playlist:
        try:
            vec.append(model.wv[song_id])
        except KeyError:
            continue
    return np.mean(vec, axis=0)
```

```{python}
playlist_vec = list(map(meanVectors, playlist_test))
```

For each playlist vector, we can recommend top $n$ similar songs based on the cosine similarity. Let's test the song embeddings to recommend top 10 songs for `playlist_test` in index `305`.

```{python}
def similarSongsByVector(vec, n = 10, by_name = True):
    # extract most similar songs for the input vector
    similar_songs = model.wv.similar_by_vector(vec, topn = n)
    
    # extract name and similarity score of the similar products
    if by_name:
        similar_songs = [(songs.loc[song_id, "artist - title"], sim)
                              for song_id, sim in similar_songs]
    
    return similar_songs
```

```{python, include=F}
def print_recommended_songs(idx, n):
    print("============================")
    print("SONGS PLAYLIST")
    print("============================")
    for song_id in playlist_test[idx]:
        print(songs.loc[song_id, "artist - title"])
    print()
    print("============================")
    print(f"TOP {n} RECOMMENDED SONGS")
    print("============================")
    for song, sim in similarSongsByVector(playlist_vec[idx], n):
        print(f"[Similarity: {sim:.3f}] {song}")
    print("============================")
```

```{python}
print_recommended_songs(idx = 305, n = 10)
```

Interestingly, the model is able to capture and recommend new songs based on the "Spanish" genre from `playlist_test` indexed at `305` without being explicitly stated. Great! The final step is to evaluate how this recommender performs.

# Evaluation

One way to evaluate the performance of a recommender system is by computing **hit rate** as follows:

1. For each song in a playlist, intentionally **Leave-One-Out (LOO)** this song.
2. By using several systems below, try to guess the LOO song.
3. Ask the recommender for top $n$ recommended songs.
4. If the LOO song appears in the top $n$ recommendation, consider it as a **HIT**. Otherwise not.
5. Repeat the LOO process until the end of the playlist. Then, the hit rate of a playlist is calculated by dividing the number of HIT with the length of a playlist.
6. Repeat step 1-5 for all playlist in `playlist_test` and calculate the **Average Hit Rate at $n$ (AHR@n)**.

```{python}
top_n_songs = 25
```

## Random Recommender

As a baseline, let's try to guess the LOO song randomly without any system.

```{python}
def hitRateRandom(playlist, n_songs):
    hit = 0
    for i, target in enumerate(playlist):
        random.seed(i)
        recommended_songs = random.sample(list(songs.index), n_songs)
        hit += int(target in recommended_songs)
    return hit/len(playlist)
```

```{python, eval=F}
eval_random = pd.Series([hitRateRandom(p, n_songs = top_n_songs)
                         for p in tqdm(playlist_test, position=0, leave=True)])
eval_random.mean()
```

> Output: 0.00030413731380910425

## Song Tags Recommender

It is possible to recommend based on song tags provided on the data as follows:

1. Create a list of song `tag_names` that surrounds the LOO song. The maximum distance between the LOO and context songs is defined by `window`.
2. List all possible songs from the list.
3. Take $n$ songs randomly from the possible songs list.

```{python}
mapping_tag2song = songs.explode('tag_names').reset_index().groupby('tag_names')['song_id'].apply(list)

def hitRateContextSongTag(playlist, window, n_songs):
    hit = 0
    context_target_list = [([playlist[w] for w in range(idx-window, idx+window+1)
                             if not(w < 0 or w == idx or w >= len(playlist))], target)
                           for idx, target in enumerate(playlist)]
    for i, (context, target) in enumerate(context_target_list):
        context_song_tags = set(songs.loc[context, 'tag_names'].explode().values)
        possible_songs_id = set(mapping_tag2song[context_song_tags].explode().values)
        
        random.seed(i)
        recommended_songs = random.sample(possible_songs_id, n_songs)
        hit += int(target in recommended_songs)
    return hit/len(playlist)
```

```{python, eval=F}
eval_song_tag = pd.Series([hitRateContextSongTag(p, model.window, n_songs = top_n_songs)
                           for p in tqdm(playlist_test, position=0, leave=True)])
eval_song_tag.mean()
```

> Output: 0.0005425495180688559

## Cluster-based Recommender

To improve further, let's utilize the result of clustering in the modelling section:

1. Identify which cluster number is the most frequent (by majority voting) in surrounding songs. The maximum distance between the LOO and context songs is defined by `window`.
2. List out possible songs from that majority cluster.
3. Take $n$ songs randomly from the possible songs list.

```{python}
def hitRateClustering(playlist, window, n_songs):
    hit = 0
    context_target_list = [([playlist[w] for w in range(idx-window, idx+window+1)
                             if not(w < 0 or w == idx or w >= len(playlist))], target)
                           for idx, target in enumerate(playlist)]
    for context, target in context_target_list:
        cluster_numbers = skm_opt.predict([model.wv[c] for c in context if c in model.wv.vocab.keys()])
        majority_voting = stats.mode(cluster_numbers).mode[0]
        possible_songs_id = list(songs_cluster[songs_cluster['cluster'] == majority_voting].index)
        recommended_songs = random.sample(possible_songs_id, n_songs)
        songs_id = list(zip(*recommended_songs))[0]
        hit += int(target in songs_id)
    return hit/len(playlist)
```

```{python, eval=F}
eval_clust = pd.Series([hitRateClustering(p, model.window, n_songs = top_n_songs)
                           for p in tqdm(playlist_test, position=0, leave=True)])
eval_clust.mean()
```

> Output: 0.005054657281168753

## Song2Vec Recommender

Lastly, evaluate the CBOW Song2Vec model as follows:

1. Take the average vectors of surrounding context songs using previously defined `meanVectors()` function. The maximum distance is defined by `window`.
2. Find top $n$ similar songs based on cosine similarity using `similarSongsByVector()` function.

```{python}
def hitRateSong2Vec(playlist, window, n_songs):
    hit = 0
    context_target_list = [([playlist[w] for w in range(idx-window, idx+window+1)
                             if not(w < 0 or w == idx or w >= len(playlist))], target)
                           for idx, target in enumerate(playlist)]
    for context, target in context_target_list:
        context_vector = meanVectors(context)
        recommended_songs = similarSongsByVector(context_vector, n = n_songs, by_name = False)
        songs_id = list(zip(*recommended_songs))[0]
        hit += int(target in songs_id)
    return hit/len(playlist)
```

```{python, eval=F}
eval_song2vec = pd.Series([hitRateSong2Vec(p, model.window, n_songs = top_n_songs)
                           for p in tqdm(playlist_test, position=0, leave=True)])
eval_song2vec.mean()
```

> Output: 0.11958469298590102

## Comparison

Finally, we compare the calculated Average Hit Rate at 25 (AHR@25) of the four recommender systems. The higher the AHR, the better is the system. From the bar plot below, Song2Vec outperforms other methods in terms of hit rate, which means that it can recommend a song well based on surrounding context songs. In a real-life scenario, this system may likely to be low quality since the AHR is only around 12%, but still, it is much better than no recommender system at all.

```{python, eval=F, include=F}
eval_df = pd.concat([eval_random.rename("Random"),
           eval_song_tag.rename("Song Tag"),
           eval_clust.rename("Clustering"),
           eval_song2vec.rename("Song2Vec")], axis = 1)
           
save2Pickle(eval_df, "eval_df")
```

```{python, echo=F, results='hide', out.width = '80%'}
eval_df = loadPickle("eval_df")
val = eval_df.mean().sort_values()

plt.figure(figsize=(15, 8))
g = val.plot(kind = 'barh', fontsize = 20)
g.set_xlabel("Average Hit Rate", fontsize = 20)
g.set_title("Recommender Evaluation AHR@25", fontweight = "bold", fontsize = 25)
for i, v in enumerate(val):
    pos = 0.5*v if i==len(val)-1 else 1.1*v
    g.text(pos, i, str(round(v, 5)), color = "black", fontweight = "bold", fontsize = 20)
plt.tight_layout()
plt.show()
```

# Conclusion

Song2Vec is an implementation of Word2Vec which able to capture the context of a song based on surrounding songs in a playlist. In this article, we successfully exploit the sequential property of a playlist and represent each song with a 256-dimensional vector. This vector representation is then used as a recommender system based on cosine similarity score. The objective of a music recommender is to create accurate personalized recommendations from historical playlist or listening queue. Therefore, metric such as AHR@n is used to evaluate how many times (on average) a song is listed on the top-$n$ recommended songs based on surrounding context songs.

Things to be taken carefully when applying Song2Vec on its own is the **cold start problem**, a condition where it is impossible to recommend any songs to a new user or even recommend a new song to any users. This can be efficiently handled by combining the recommender using a content-based technique, which utilizes explicit features or characteristics of the songs as demonstrated in the "Song Tags Recommender" section.

Maybe you're wondering what are other implementations of Word2Vec? Here is the list for you: 

- Product recommendations: Using purchase receipts in a transaction to capture an item embeddings to learn the user's purchase activity.
- Listing recommendations: The user activity is in the form of click data, which can be represented as a sequence of listings that a user viewed.
- Matching advertisement to search query: Data consist of sequential search sessions, including entered query, clicked advertisement, and search results.

> The full Jupyter Notebook is available on my [Github](https://github.com/tomytjandra/song2vec-music-recommender) 

# Annotation

[^1]: [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/pdf/1310.4546.pdf)

[^2]: [Playlist Dataset](https://www.cs.cornell.edu/~shuochen/lme/data_page.html)

[^3]: [word2vec Parameter Learning Explained](https://arxiv.org/pdf/1411.2738.pdf)

[^4]: [word2vec: 
Tool for computing continuous distributed representations of words](https://code.google.com/archive/p/word2vec/)

[^5]: [gensim: Topic modelling for humans](https://radimrehurek.com/gensim/)

[^6]: [Fast Single- and Cross-Show Speaker Diarization Using Binary Key Speaker Modeling](https://www.researchgate.net/figure/Example-of-the-elbow-criterion-applied-over-the-curve-of-within-class-sum-of-squares-per_fig1_282000605)

[^7]: [Using Word2Vec for Music Recommendations](https://towardsdatascience.com/using-word2vec-for-music-recommendations-bb9649ac2484)